##########################################
# Robinhood configuration file template  #
##########################################

# global configuration
General
{
    # filesystem to be monitored
    fs_path = "{{ rh_mountpoint }}" ;

    # filesystem type (as returned by 'df' or 'mount' commands)
    fs_type = "{{ test_fs }}" ;

    # filesystem property used as FS key: fsname, devid or fsid (fsid NOT recommended)
    fs_key = fsname ;

    # check that objects are in the same device as 'fs_path',
    # so it will not traverse mount points
    stay_in_fs = yes ;

    # check that the filesystem is mounted
    check_mounted = yes ;

    # Set the last_access time by only the atime variable, and not MAX(atime,mtime)
    # There are no guarantees that all filesystems will correctly store atime
    last_access_only_atime = no ;
    uid_gid_as_numbers = no ;
}

# logs configuration
Log
{
    # Log verbosity level
    # Possible values are: CRIT, MAJOR, EVENT, VERB, DEBUG, FULL
    debug_level = EVENT ;

    # Log file
    log_file = "/var/log/robinhood.log" ;

    # File for reporting purge events
    report_file = "/var/log/robinhood_actions.log" ;

    # set alert_file, alert_mail or both depending on the alert method you wish
    alert_file = "/var/log/robinhood_alerts.log" ;
    alert_mail = "root@localhost" ;


    # Interval for dumping stats (to logfile)
    stats_interval = 20min ;

    # Alert batching (to send a digest instead of 1 alert per file)
    # 0: unlimited batch size, 1: no batching (1 alert per file),
    # N>1: batch N alerts per digest
    batch_alert_max = 5000 ;
    # Give the detail of entry attributes for each alert?
    alert_show_attrs = no ;

    # whether the process name appears in the log line
    log_procname = yes;
    # whether the host name appears in the log line
    log_hostname = yes;
}

# updt params configuration
db_update_params
{
    # possible policies for refreshing metadata and path in database:
    #   never: get the information once, then never refresh it
    #   always: always update entry info when processing it
    #   on_event: only update on related event
    #   periodic(interval): only update periodically
    #   on_event_periodic(min_interval,max_interval)= on_event + periodic

    # Updating of file metadata
    md_update = always ;
    # File classes matching
    fileclass_update = always ;
}

# list manager configuration
ListManager
{
    # Method for committing information to database.
    # Possible values are:
    # - "autocommit": weak transactions (more efficient, but database inconsistencies may occur)
    # - "transaction": manage operations in transactions (best consistency, lower performance)
    # - "periodic(<nb_transaction>)": periodically commit (every <n> transactions).
    commit_behavior = transaction ;

    # Minimum time (in seconds) to wait before trying to reestablish a lost connection.
    # Then this time is multiplied by 2 until reaching connect_retry_interval_max
    connect_retry_interval_min = 1 ;
    connect_retry_interval_max = 30 ;
    # disable the following options if you are not interested in
    # user or group stats (to speed up scan)
    accounting  = enabled ;

    MySQL
    {
        server = "localhost" ;
        db     = "{{ database_name }}" ;
        user   = "robinhood" ;
        password_file = "/etc/robinhood.d/.dbpassword" ;
        # port   = 3306 ;
        # socket = "/tmp/mysql.sock" ;
        engine = InnoDB ;
    }
}

# entry processor configuration
EntryProcessor
{
    # nbr of worker threads for processing pipeline tasks
    nb_threads = {{ thread_count }} ;

    # Max number of operations in the Entry Processor pipeline.
    # If the number of pending operations exceeds this limit, 
    # info collectors are suspended until this count decreases
    max_pending_operations = 100 ;

    # max batched DB operations (1=no batching)
    max_batch_size = 100;

    # Optionnaly specify a maximum thread count for each stage of the pipeline:
    # <stagename>_threads_max = <n> (0: use default)
    # STAGE_GET_FID_threads_max	= 4 ;
    # STAGE_GET_INFO_DB_threads_max	= 4 ;
    # STAGE_GET_INFO_FS_threads_max	= 4 ;
    # STAGE_PRE_APPLY_threads_max	= 4 ;
    # Disable batching (max_batch_size=1) or accounting (accounting=no)
    # to allow parallelizing the following step:
    # STAGE_DB_APPLY_threads_max	= 4 ;

    # if set to 'no', classes will only be matched
    # at policy application time (not during a scan or reading changelog)
    match_classes = yes;

    # Faking mtime to an old time causes the file to be migrated
    # with top priority. Enabling this parameter detect this behavior
    # and doesn't allow  mtime < creation_time
    detect_fake_mtime = no;
}

# FS scan configuration
FS_Scan
{
    # simple scan interval (fixed)
    scan_interval      =   6h ;

    # min/max for adaptive scan interval:
    # the more the filesystem is full, the more frequently it is scanned.
    #min_scan_interval      =    2h ;
    #max_scan_interval      =   12h ;

    # number of threads used for scanning the filesystem
    nb_threads_scan        =     2 ;

    # when a scan fails, this is the delay before retrying
    scan_retry_delay       =    1h ;

    # timeout for operations on the filesystem
    scan_op_timeout        =    1h ;
    # exit if operation timeout is reached?
    exit_on_timeout        =    yes ;
    # external command called on scan termination
    # special arguments can be specified: {cfg} = config file path,
    # {fspath} = path to managed filesystem
    #completion_command     =    "/path/to/my/script.sh -f {cfg} -p {fspath}" ;

    # Internal scheduler granularity (for testing and of scan, hangs, ...)
    spooler_check_interval =  1min ;

    # Memory preallocation parameters
    nb_prealloc_tasks      =   256 ;

    Ignore
    {
        # ignore ".snapshot" and ".snapdir" directories (don't scan them)
        type == directory
        and
        ( name == ".snapdir" or name == ".snapshot" )
    }
}

#### policy definitions ####

# includes go here
%include "vagrant_test_policy.inc"
